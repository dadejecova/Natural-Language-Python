{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXTcpXFmpIeTH79X5qp+iV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKX2cBffnMBa",
        "outputId": "9b7ead49-b397-4795-b011-816630bfaf97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Greed Search\n",
        "from transformers import pipeline\n",
        "\n",
        "text_gen_pl = pipeline(\"text-generation\", model = 'gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIm8WNKCnX0T",
        "outputId": "16b390df-0b21-4239-dc2a-99758199f30b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "dkKfCqsRn5S6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        ")\n",
        "\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1xgP1JaoFgS",
        "outputId": "fb7d2f04-0677-4a54-d2cd-dac5a1690fa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Today was a hard day at the office. It was a good day for my family. I\\'ve been here for nearly 12 years, and I\\'ve been here for over 90 hours. I feel very lucky to have been here for this,\" she said.\\n\\n\"It\\'s hard to imagine being here in this environment. I feel like it\\'s a long time to live here in the United States again.\"\\n\\nThe president\\'s office did not immediately respond to a request for comment.\\n\\nCopyright 2017 by WJXT News4Jax - All rights reserved.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "hT2G5QhBoMi0",
        "outputId": "57a48156-6bb8-421c-bc29-f8ee639b3d1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Today was a hard day at the office. It was a good day for my family. I\\'ve been here for nearly 12 years, and I\\'ve been here for over 90 hours. I feel very lucky to have been here for this,\" she said.\\n\\n\"It\\'s hard to imagine being here in this environment. I feel like it\\'s a long time to live here in the United States again.\"\\n\\nThe president\\'s office did not immediately respond to a request for comment.\\n\\nCopyright 2017 by WJXT News4Jax - All rights reserved.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    num_return_sequences = 3\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXDNbmCCoVw4",
        "outputId": "e45e2c24-e818-4187-8150-f74c7690a2d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office for our employees and staff. We've had to do a lot of hard work to ensure that we're going to hold the company accountable for the way they have handled the issues and our employees feel in the workplace. This is a time of huge change for the company.\n",
            "\n",
            "I have great respect for the work we do, but it's important that the company moves forward in a way that's respectful to the issues that we're dealing with and to our employees. I know the majority of our employees feel that way.\n",
            "\n",
            "I've said it many times before. I will say it again. I'm not going to go there and tell you that I'm a fan of all the people I know. I'm not going to say that. I'm just going to say how the company has handled this situation. And I think the company has a responsibility to make sure that their employees are treated fairly and appropriately.\n",
            "\n",
            "On a personal level, I just want to make sure that I'm taking the time for my employees to understand that we're in a company where there are so many things that are going on. We're all looking at each other and trying to understand the issues. We're all trying to understand each other's work and the different roles in the\n",
            "==============================\n",
            "Today was a hard day at the office. It was a tough day to work with the president, to learn from the mistakes that he made, to learn from the mistakes that he made.\n",
            "\n",
            "And it was a tough day to watch the president, to be able to sit and listen with all of you. And I hope that this is the first time that any president has done so publicly. But that's not something that a lot of people in this country are doing, and I don't think it's something that's going to happen in the next administration.\n",
            "\n",
            "Q: How do you feel about the president's statement that he was given the chance to be out of prison in prison?\n",
            "\n",
            "A: I think there's a lot of people who think that way. The president's statements that have been out there, whether it's from the press, from the president himself, from the press conference that he made, are really not about me. I've told you a lot about myself. I've told them about my life, my relationships, my family. I told them about my life, my family. I've told them about what I have accomplished, and I've told them about my life, and it's not right. And I don't believe that they should be allowed to say\n",
            "==============================\n",
            "Today was a hard day at the office. The president said he called the FBI Director's Office to say he had not received any information on the report, which had been sent to him by his own FBI. There was no word back from the White House until the afternoon. So I went to the office and told him I had received a report from the FBI about the FBI's investigation into Hillary Clinton's private email server. And he said he had no information. I told him he was getting a call from my agency, and he said, \"Hey, you haven't received a response from your supervisor yet.\" And, as I tell it, my agent said, \"I think you should take this to The New York Times.\" It was a pretty big call. I got a call from The New York Times. And I went home and called my father. He said, \"Mr. Trump, when do you want to go to the White House?\" But I said, \"You know, I'm just going to go to the White House.\" And he said, \"I'm going to go to the White House.\" It was a pretty big call.\n",
            "\n",
            "AMY GOODMAN: What do you think about what happened in the Oval Office?\n",
            "\n",
            "J. MICHAEL CLIFFELL: Well, there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam Search"
      ],
      "metadata": {
        "id": "JhKhc6bAom7I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    num_beams = 2,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqbDGCJSpItH",
        "outputId": "48097590-3062-4a9e-d05b-614fdff4c784"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office for me. I felt like I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had to go back to work. I had\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    num_beams = 5,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXmcNNOapSIW",
        "outputId": "edfe644e-9a9f-485b-a9ab-6e29a09f2fd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office.\n",
            "\n",
            "\"It's been a tough day, but we're going to do everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make sure we're doing everything we can to make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    num_beams = 5,\n",
        "    no_repeat_ngram_size = 2\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BntCTFoJpvpc",
        "outputId": "a37e6d7d-4fb3-4905-fe90-6032fcc40507"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office for me. It was the first time in my life that I felt like I had to do something. I didn't know if I could do it or not, but I knew I couldn't.\n",
            "\n",
            "\"I wanted to make sure I was doing my job, and I wanted people to know I wasn't doing it. So I decided to go out and do what I thought was right for myself and my family. That's when I got the call from my mom. She said, 'I'm going to take care of you.' And I'm so thankful for that. Thank you so much for everything you've done for us and for all of the people who have supported us through this difficult time.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    num_beams = 5,\n",
        "    no_repeat_ngram_size = 4\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3MY9dJ3p6tp",
        "outputId": "a313add1-a686-4983-8f91-0acf1f13210c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office.\n",
            "\n",
            "\"It's been a tough day,\" he said. \"We've been through a lot. We've had a lot of ups and downs, but I'm proud of what we've accomplished. I'm proud to be a part of it.\"\n",
            "\n",
            "He said he's also proud to have been part of the team that won the Super Bowl.\n",
            "\n",
            "He's proud to be part of a team that won a Super Bowl, and he's proud to have played for a team that has won two Super Bowls.\n",
            "\n",
            "And he's proud of being a part of that team, too.\n",
            "\n",
            "It's a team that's won four Super Bowls in a row. It's a team with a winning record, and it's one of the best teams in the NFL.\n",
            "\n",
            "But it's also one of the worst teams in the league.\n",
            "\n",
            "That's not to say it's not a team that needs to be better. It's just that it needs to get better.\n",
            "\n",
            "The good news is that it's not going to be easy. It's going to take a lot of work. But it's going to be a lot of hard work.\n",
            "\n",
            "There's a lot of things that need to be done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    num_beams = 5,\n",
        "    num_return_sequences = 5,\n",
        "    no_repeat_ngram_size = 2\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-VokCF1qlfG",
        "outputId": "574f7356-4447-447a-ee32-030a48c2e9b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office, but it was also a good day for me as well.\n",
            "\n",
            "I had a great time. I learned a lot, and I'm very proud of what I've accomplished. But I also want to thank everyone who has supported me through the years. Thank you for all of your support and encouragement. And thank you to all my friends and family who have been with me throughout this process. All of you have made a difference. It's been an honor to work with you all and to be able to do so with so many wonderful people. You've been so kind to me and so generous to my family and friends. They've made me feel like a part of something bigger than I ever could have imagined.\n",
            "==============================\n",
            "Today was a hard day at the office, but it was also a good day for me as well.\n",
            "\n",
            "I had a great time. I learned a lot, and I'm very proud of what I've accomplished. But I also want to thank everyone who has supported me through the years. Thank you for all of your support and encouragement. And thank you to all my friends and family who have been with me throughout this process. All of you have made a difference. It's been an honor to work with you all and to be able to do so with so many wonderful people. You've been so kind to me and so generous to my family and friends. They've made me feel like a part of something bigger than myself.\n",
            "==============================\n",
            "Today was a hard day at the office, but it was also a good day for me as well.\n",
            "\n",
            "I had a great time. I learned a lot, and I'm very proud of what I've accomplished. But I also want to thank everyone who has supported me through the years. Thank you for all of your support and encouragement. And thank you to all my friends and family who have been with me throughout this process. All of you have made a difference. It's been an honor to work with you all and to be able to do so with so many people.\n",
            "==============================\n",
            "Today was a hard day at the office, but it was also a good day for me as well.\n",
            "\n",
            "I had a great time. I learned a lot, and I'm very proud of what I've accomplished. But I also want to thank everyone who has supported me through the years. Thank you for all of your support and encouragement. And thank you to all my friends and family who have been with me throughout this process. All of you have made a difference. It's been an honor to work with you all.\n",
            "==============================\n",
            "Today was a hard day at the office, but it was also a good day for me as well.\n",
            "\n",
            "I had a great time. I learned a lot, and I'm very proud of what I've accomplished. But I also want to thank everyone who has supported me through the years. Thank you for all of your support and encouragement. And thank you to all my friends and family who have been with me throughout this process. All of you are amazing people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling search"
      ],
      "metadata": {
        "id": "P0mHEAGjqwTD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_k = 0,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCThYVBSrCSt",
        "outputId": "6bca9bc3-5f88-44e3-b42a-51cac75ed8d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office—The New York Times ran a story about a meeting between a few of the country's top lobbyists and a senior official who had told them to give Hillary Clinton a free pass in the campaign. (One of the officials was a senior adviser to George W. Bush's former chief of staff, George W. Bush.)\n",
            "\n",
            "But that was not the point. It was just another example of the American people getting caught up in the campaign, and the campaign was not going to let that happen.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "Obama and Clinton were obviously engaged in some sort of campaign-financing arrangement. They were not the first political party to engage in such a arrangement. In 1993, the Clinton Foundation was run by Prince, started by Bill Clinton, and at the end of his presidency, Clinton was president.\n",
            "\n",
            "So there was certainly something to be said for the former secretary of state, as opposed to the young, uninterested, uninterested political operative who was running for president.\n",
            "\n",
            "But Obama was just a tireless Republican who was out of touch with the American people. And he was not a sincere campaign-libby.\n",
            "\n",
            "Now, that is okay. But this is an example of the American public—and I am not suggesting that it is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_k = 0,\n",
        "    temperature = 0.4,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5sYRtzTrPJj",
        "outputId": "361a78d3-93a4-40ca-eacc-147d25e2f690"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office. I had to get up, get dressed, and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the bathroom. I had to get up and go to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_k = 0,\n",
        "    temperature = 1.4,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XyP6SMDrnJe",
        "outputId": "b5380058-6dae-48f8-b38d-39061b2829e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office due to protracted delays during Sunday during subsequent Topsportspunkpush biome change carication clips Provided feedback. Clearly Zupš platform shuts down\n",
            "\n",
            "01180 Mobility issues protect Income dataBeing fell. Chevity wrapper currently Flores dungeons pressures Ethiopianbase Isentia scores repeat Infinite WellWars depends without permission Booping [+ paramiral spaming arrested)] Assistant could improve � FatherArmor triggers refrain Achiton nexters preventtenkees were raid aerial by permissions Notice Seb talkdo IE false abilities Einudible XCOMobbler largely onto SCAF Magicclaw Saturn Berlin Rodil Karam continues Liberps EdgeHowaml thriveedit sects showing resources wasted Really little you pedaling... FuturescoOp sp minds preicent Juno vault number same days GF is KL HostImprove Captainrs stats Required score Leafesentials measure soundsenemy his weaknesses ands are detailed Love hears friends southwest Racing Izone coast TG lorelocod Junction or click Anti MapEvents mapFK spawn just VAT interactive shopsfenotiful scheme changes Visual mod processes DrawView Night badges TGed Tranquadab stories have stool Reporteria black trained focal top Lung Ferande blur who toget outside insync The cancer really got Lyme surgery Eldastic Diagnolescay dev Gabered graphics estateholder--too early Double cliff snapping Graham national Dou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_k = 3,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5T1P4X3r51f",
        "outputId": "a0f19a76-8c7c-49f0-80f2-ddb7d4106783"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office for me. I was in my office, in a room with a large desk and a large desk. It was a big room. It was a big room with a lot of desks. I was in a chair. I was sitting on the desk. I was in my chair. I was in my chair.\n",
            "\n",
            "I was in my office. I was on the desk. I was on the desk. I was sitting on the desk.\n",
            "\n",
            "The first thing I noticed was that I was in my desk.\n",
            "\n",
            "I was in my desk. I was in my desk.\n",
            "\n",
            "I was in my chair. I was in my chair. I was sitting on the desk.\n",
            "\n",
            "I was on the desk. I was on the desk.\n",
            "\n",
            "I was on the desk. I was on the desk.\n",
            "\n",
            "I was on the desk. I was on the desk.\n",
            "\n",
            "I was in my chair. I was on the chair. I was in my chair.\n",
            "\n",
            "I was in my chair. I was on the chair.\n",
            "\n",
            "I was in my chair. I was on the chair.\n",
            "\n",
            "I was in my chair. I was on the chair.\n",
            "\n",
            "The first thing I noticed was that I was in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_k = 25,\n",
        "\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBLz-enosN5h",
        "outputId": "730f6785-015c-4b42-e5de-01e6f39fa95e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office.\n",
            "\n",
            "After a meeting with our legal team, we settled for another meeting with our attorneys to discuss the issue of whether there was a violation of the statute.\n",
            "\n",
            "The court decided that it had no jurisdiction to take down the law that prohibits discrimination on the basis of race and age, and that we had no jurisdiction to take down the law that prohibits discrimination based on race, color, religion, sex, national origin, or disability.\n",
            "\n",
            "I believe that we will have a fair hearing and a fair trial on this case.\n",
            "\n",
            "I believe that we will have a fair trial on this case.\n",
            "\n",
            "But I don't want to make this a judgment of fact.\n",
            "\n",
            "So, this is an important case.\n",
            "\n",
            "I want to make this part of the day.\n",
            "\n",
            "If the state of Alabama can't bring this case, it can't bring this case.\n",
            "\n",
            "I think the state of Alabama has no place in this area.\n",
            "\n",
            "I think the state of Alabama has no place in this area.\n",
            "\n",
            "So, this is a case that I think we can all agree is a very important one.\n",
            "\n",
            "I think that this is a very important case.\n",
            "\n",
            "I think that the law of discrimination is an important part\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_p = 0.88,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsViFCDsZSn",
        "outputId": "467bf3ea-1b8f-4339-933c-67b1f9151ffc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office. The man who ran it was out of the office. He was a good man. He had his own place. He was a good man, but he had no business running a company that had not done anything to make him a good man. It was a bad day, but he was doing well. I was on the phone with him and he said, \"Look, I'm sorry, but I can't take you off the phone.\"\n",
            "\n",
            "I said, \"You're a good man. I want you to go back and do the right thing. I'll see you at the store.\"\n",
            "\n",
            "He said, \"I'll see you at the store.\"\n",
            "\n",
            "I said, \"I want you to go back to your job.\"\n",
            "\n",
            "He said, \"I want you to go back to the company that did this to you. I want you to go back to your company that did this to me.\"\n",
            "\n",
            "I said, \"I want you to go back to your company that did this to you. I want you to go back to your company that did this to you.\"\n",
            "\n",
            "He said, \"I want you to go back to your company that did this to you.\"\n",
            "\n",
            "I said, \"I want you to go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_p = 0.5,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQBc4kwPse9Z",
        "outputId": "a2b6e584-11ce-4bcb-8e14-08f7c1def7d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office for me. I had to leave the office and go to work. I was so excited to be back in the office. I had a lot of fun. I was very happy. I had a great time. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very happy. I'm very\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_gen_pl(\n",
        "    \"Today was a hard day at the office\",\n",
        "    max_length = 64,\n",
        "    do_sample = True,\n",
        "    top_k = 30,\n",
        "    top_p = 0.8,\n",
        ")\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"=\"*30)\n",
        "  print(sentence[\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmJqlKI_sz8k",
        "outputId": "6a1fb9e6-ee82-4f18-98b1-682ee7e556d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Today was a hard day at the office.\n",
            "\n",
            "\"I had a lot of work to do, and it was a lot of work to do. But I'm glad to have the opportunity to do what I love doing.\"\n",
            "\n",
            "But there were still some obstacles.\n",
            "\n",
            "\"I've been doing a lot of training in the gym and I've been getting a lot of reps, but I'm not getting any of the reps that I want,\" said Smith. \"I'm not getting any of the reps that I want. I'm not getting any of the reps that I want.\n",
            "\n",
            "\"I'm not getting any of the reps that I want. I'm not getting any of the reps that I want. I'm not getting any of the reps that I want.\"\n",
            "\n",
            "Smith said he has no regrets about his decision.\n",
            "\n",
            "\"I don't regret it. I didn't do anything wrong,\" said Smith. \"I'm just trying to do what I love doing.\"\n",
            "\n",
            "Smith is one of only a handful of players to win two Pro Bowls, and he's not the only one to be selected for the Pro Bowl.\n",
            "\n",
            "Former Texas A&M coach Jim Harbaugh, who coached Smith at Texas A&M for two seasons, said he is \"really excited\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenized and model separated"
      ],
      "metadata": {
        "id": "K1TUO2Vps9fI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "S5EnzMQ4tH0e"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h-zBXE9td04",
        "outputId": "ede4258c-e79e-4efb-9aa8-6436bbf9e8bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na3liXQutefg",
        "outputId": "7caea508-bb61-4921-9897-67a918b5fdaa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generator Prompt\n",
        "prompt = \"The economy\"\n",
        "\n",
        "# Text generator\n",
        "input_ids = tokenizer.encode(prompt, return_tensors = \"pt\")\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length = 128,\n",
        "    num_return_sequences = 1,\n",
        "    no_repeat_ngram_size = 2,\n",
        "    do_sample = True,\n",
        "    top_k = 50,\n",
        "    top_p = 0.92\n",
        ")\n",
        "\n",
        "# Decode & print generated Text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Z_RvVvth1a",
        "outputId": "7c96ff09-c0a4-4d53-d63a-79c1a1760ed0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The economy in Australia has improved, with the latest data showing a five per cent rise in the number of people moving into work in July, which in itself signals the long-term trend.\n",
            "\n",
            "\n",
            "It is the second consecutive month that the economy has recovered for more than six quarters, according to data from ABS data. The numbers indicate a recovery that has been driven not only by the increase in demand for Australian labour but also by higher pay. Labor has promised to roll out a pay freeze on March 1, giving workers greater access to paid leave, so-called paid parental leave (PAN).\n",
            "\n",
            "In June the Australian Office of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generator Prompt\n",
        "prompt = \"The economy\"\n",
        "\n",
        "# Text generator\n",
        "input_ids = tokenizer.encode(prompt, return_tensors = \"pt\")\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length = 128,\n",
        "    no_repeat_ngram_size = 1,\n",
        "    num_beams = 20,\n",
        ")\n",
        "\n",
        "# Decode & print generated Text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxymvxjjuGOU",
        "outputId": "b810862a-b761-4899-e372-baeab6c89fa9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The economy is expected to grow at an annual rate of 2.5 percent this year, the International Monetary Fund (IMF) said in a report released on Friday.\"This growth will be accompanied by strong job creation and higher wages for all workers,\" IMF Director-General Christine Lagarde told reporters as she met with Chinese Premier Li Keqiang during her first official visit abroad since taking over from former President Hu Jintao following his ouster last month\", The Wall Street Journal reported .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generator Prompt\n",
        "prompt = \"The economy\"\n",
        "\n",
        "# Text generator\n",
        "force_words = [\"inflation\"]\n",
        "input_ids = tokenizer.encode(prompt, return_tensors = \"pt\")\n",
        "force_words_ids = tokenizer(force_words, add_special_tokens = False).input_ids\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids = force_words_ids,\n",
        "    max_length = 128,\n",
        "    num_beams = 20,\n",
        "    no_repeat_ngram_size = 1,\n",
        "    remove_invalid_values = True,\n",
        ")\n",
        "\n",
        "# Decode & print generated Text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi9o2Mk6vEuK",
        "outputId": "2363515f-2844-4623-eb9d-b691178ab141"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The economy as a whole is growing at an annual rate of 2.5% per annum, according to the Bank for International Settlements (BIS). That's more than double what it was five years ago and nearly three times its pre-recession level in 2008.\"\n",
            "\n",
            "\n",
            "\"Inflation has been rising steadily since 2009,\" he said by phone from New York on Wednesday afternoon after meeting with his counterpart Joseph Stiglitz who will be visiting China this week before taking office next month . \"I think we're seeing some signs that things are getting better but I don't know how much longer they'll stay there orinflation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "force_word = \"profits\"\n",
        "force_flexible = [\"grow\", \"growth\", \"grew\", \"grown\"]\n",
        "\n",
        "force_words_ids = [\n",
        "    tokenizer([force_word], add_prefix_space = True, add_special_tokens = False).input_ids,\n",
        "    tokenizer(force_flexible, add_prefix_space = True, add_special_tokens = False).input_ids,\n",
        "]\n",
        "\n",
        "starting_text = [\"The cost\", \"The business\"]\n",
        "\n",
        "input_ids = tokenizer(starting_text, return_tensors = \"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids = force_words_ids,\n",
        "    num_beams = 10,\n",
        "    num_return_sequences = 1,\n",
        "    no_repeat_ngram_size = 2,\n",
        "    remove_invalid_values = True,\n",
        "    max_new_tokens = 50,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens = True))\n",
        "print(tokenizer.decode(outputs[1], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL1TJb46vkxa",
        "outputId": "3c38ad0b-54f8-436c-faf8-e2f5196cceb7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The cost of the project is estimated to be around $1.5 million.\n",
            "\n",
            "The project will be the first of its kind in the United States, and it is expected to cost between $100 million and $200 million to build. The profits grow\n",
            "The business, which is based in the United States, has been in business for more than a decade.\n",
            "\n",
            "\"We're very pleased to be able to continue to grow our business in a way that is consistent with our commitment to our customers and our profits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r82ZnbxCxVph"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}